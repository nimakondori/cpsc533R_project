train: 
  seed: 200
  batch_size: 16 
  num_epochs: 200  
  num_workers: 8
  use_wandb: False
  wandb_project_name: MultiTasking_Transformer
  wandb_entity: rcl_stroke
  wandb_mode: online
  wandb_run_name: "debug_run_pretrained"

  lr_schedule: 
    name: "reduce_lr_on_plateau"
    mode: 'min'
    factor: 0.5  # Factor by which the learning rate will be reduced
    patience: 2 # Number of epochs with no improvement after which learning rate will be reduced
    threshold: 0.01  # Threshold for measuring the new optimum, to only focus on significant changes
    min_lr: 0.000001  # 1e-6
    verbose: True

  optimizer: 
    name: adam
    lr: 0.001  # 1e-3
    weight_decay: 0.0001  # 1e-4

  criterion:
    mae:
      loss_weight: 1 # Coefficient in weighted loss calculation    

model:
  name: pretrained
  checkpoint_path: ""    
  n_classes: 8
  model_name: "google/vit-base-patch16-224"


eval:  
  standards: ["mse", "landmarkcoorderror"]  
  standard: "mse"  
  best_mode: "min" # min or max
  minimize: False

data: 
  - dataset:
      name: lvidlandmark
      data_dir: '/mnt/data/LV_PLAX2_cleaned/Cleaned'
      metadata_dir: '/mnt/data/lv_plax2_cleaned_info_landmark_gt_filtered.csv'
      transform: 
        image_size: 224
        crop_size: 28
